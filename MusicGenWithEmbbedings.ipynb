{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicGenWithEmbbedings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsNUS8SFuoirdPo6lgjtJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoranReznik/MusicGenerator/blob/main/MusicGenWithEmbbedings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUZnpbRU4vs"
      },
      "source": [
        "# install requeird libreries\n",
        "!pip install mido\n",
        "\n",
        "# imports\n",
        "import os\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import mido\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# needed only for MAC\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# creating cude device to run on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device: {}'.format(device))\n",
        "if str(device) == 'cuda':\n",
        "  print(torch.cuda.get_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdRG3VbTVUtW"
      },
      "source": [
        "# unzipping the file with the music pieces\n",
        "!unzip /content/MusicMatrices1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLmoqCAlVtHx"
      },
      "source": [
        "# creating the vocabulary\n",
        "vocab = []\n",
        "path = '/content/MusicMatrices'\n",
        "pieces = {}\n",
        "for i in os.listdir(path):\n",
        "  if '.npy' in i:\n",
        "    mat = np.load(path+'/'+i)\n",
        "    pieces[i] = mat\n",
        "    l = mat.tolist()\n",
        "    for j in l:\n",
        "      # convert to array, remove length (was in position 89 at each vector)\n",
        "      # and set all note velocity to same value\n",
        "      j = np.array(j)\n",
        "      j[0:88][j[0:88] > 0] = 50\n",
        "      j = j[0:88]\n",
        "      j = list(j)\n",
        "\n",
        "      vocab.append(str(j))\n",
        "vocab = list(set(vocab))\n",
        "print('number of unique combination of notes (\"words\"): '+str(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZEWPw5rXADg"
      },
      "source": [
        "# transition dicts from word to vec and back dicts\n",
        "w2i = {}\n",
        "i2w = {}\n",
        "for i, word in enumerate(vocab):\n",
        "  w2i[word] = i\n",
        "  i2w[i] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEVjPtIrSvTU"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\"create a torch dataset to load the data into the model during training\"\"\"\n",
        "\n",
        "  def __init__(self, seq_len, n_pieces=250):\n",
        "    \"\"\"initiate the dataset\"\"\"\n",
        "\n",
        "    # initate class variables\n",
        "    self.seq_len = seq_len # length of dequence fed into the model\n",
        "    self.lengths = {} # number of training examples in each music piece\n",
        "    self.examlpe_num = 0 # number of total trainig examples in dataset\n",
        "\n",
        "    # calulate values for self.lengths and self.examlpe_num\n",
        "    for piece_name in os.listdir('/content/MusicMatrices/')[:n_pieces]:\n",
        "      if 'npy' in piece_name:\n",
        "        piece = pieces[piece_name]\n",
        "        self.lengths[piece_name] = piece.shape[0] - (self.seq_len+1)\n",
        "        self.examlpe_num += piece.shape[0] - (self.seq_len+1)\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return self.examlpe_num\n",
        "\n",
        "  def __getitem__(self, example_idx):\n",
        "        \"\"\"retirives a training example given idx\"\"\"\n",
        "        \n",
        "        # deduce the piece that contains the index by iterating over their number of training examples\n",
        "        s = 0\n",
        "        for piece_check in self.lengths:\n",
        "          if s + self.lengths[piece_check] > example_idx:\n",
        "            piece = piece_check\n",
        "            example_idx -= s\n",
        "            break\n",
        "          s += self.lengths[piece_check]\n",
        "\n",
        "        # load the piece and exctract training example\n",
        "        piece = pieces[piece]\n",
        "        x = piece[example_idx:example_idx+self.seq_len,:]\n",
        "        y = piece[example_idx+1:example_idx+self.seq_len+1,:]\n",
        "\n",
        "        # convert to array, remove length (was in position 89 at each vector)\n",
        "        # and set all note velocity to same value\n",
        "        x[:,0:88][x[:,0:88] > 0] = 50\n",
        "        y[:,0:88][y[:,0:88] > 0] = 50\n",
        "        x = x[:,0:88]\n",
        "        y = y[:,0:88]\n",
        "\n",
        "        # convert the a list of the words' indexes \n",
        "        x = [str(j) for j in x.tolist()]\n",
        "        y = [str(j) for j in y.tolist()]\n",
        "        x = [w2i[j] for j in x]\n",
        "        y = [w2i[j] for j in y]\n",
        "\n",
        "        # convert to tensors, and create one hot encoding to the target words\n",
        "        x = torch.tensor(x).long()\n",
        "        y = torch.tensor(y).long()\n",
        "        y = nn.functional.one_hot(y, num_classes=len(vocab))\n",
        "        \n",
        "        return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2_VPImWYZp8"
      },
      "source": [
        "class AttentionError(Exception):\n",
        "    pass\n",
        "\n",
        "class MultiheadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Narrow multiheaded attention. Each attention head inspects a \n",
        "    fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, sequence_len, heads=8, dropout=0.1):\n",
        "        \"\"\"initiate the layer\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # make sure embedding size is devidable by number of heads\n",
        "        if d_model % heads != 0:\n",
        "            raise AttentionError(\"Number of heads does not divide model dimension\")\n",
        "\n",
        "        # initate class variables\n",
        "        self.d_model = d_model # embedding size\n",
        "        self.heads = heads # number of heads\n",
        "        s = d_model // heads # features per head\n",
        "        self.linears = []\n",
        "        for head in range(heads):\n",
        "          self.linears.append(torch.nn.ModuleList([nn.Linear(s, s, bias=False) for i in range(3)])) # linear layers: WQ, WK and WV\n",
        "        self.recombine_heads = nn.Linear(heads * s, d_model) # combines heads back to single output of the right size\n",
        "        self.dropout = nn.Dropout(p=dropout) # dropout layer\n",
        "        self.Er = torch.randn([s, sequence_len], device=device, requires_grad=True) # relative positinal embedding matrix\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_mask = (x != 0)\n",
        "        mask = x_mask.type(torch.uint8)\n",
        "        x = x.permute(1,0,2)\n",
        "        #batch size, sequence length, embedding dimension\n",
        "        b, t, e = x.size()\n",
        "        #each head inspects a fraction of the embedded space\n",
        "        h = self.heads\n",
        "        #single head dimension\n",
        "        s = e // h\n",
        "        # devide input to seperate dim for each head \n",
        "        x = x.view(b,t,h,s)\n",
        "        # compute seperate queries, keys and values for each head\n",
        "        queries, keys, values = [], [], []\n",
        "        for head in range(self.heads):\n",
        "          x_head = x[:,:,head,:]\n",
        "          q, k, v = [w(x) for w, x in zip(self.linears[head].to(device), (x_head,x_head,x_head))]\n",
        "          queries.append(q)\n",
        "          keys.append(k)\n",
        "          values.append(v)\n",
        "\n",
        "        #apply same position embeddings across the batch\n",
        "        SRel = []\n",
        "        for head in range(self.heads):\n",
        "          QEr = torch.matmul(queries[head], self.Er)\n",
        "          SRel.append(QEr.contiguous().view(b, t, t))\n",
        " \n",
        "        #Compute scaled dot-product self-attention   \n",
        "        head_represenations = []\n",
        "        for head in range(self.heads):\n",
        "          # scale pre-matrix multiplication for stability\n",
        "          queries[head] = queries[head] / (e ** (1/4))\n",
        "          keys[head] = keys[head] / (e ** (1/4))\n",
        "          # multiply queries by keys\n",
        "          scores_head = torch.bmm(queries[head], keys[head].transpose(1, 2))\n",
        "          # add atantion socres to relative positional scores\n",
        "          scores = scores_head + SRel[head]   \n",
        "          # mask \n",
        "          subsequent_mask = torch.triu(torch.ones(1, t, t, device=device), 1)\n",
        "          scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n",
        "          #Convert scores to probabilities\n",
        "          attn_probs = F.softmax(scores, dim=2)\n",
        "          attn_probs = self.dropout(attn_probs)\n",
        "          #use attention to get a weighted average of values\n",
        "          head_represenations.append(torch.bmm(attn_probs, values[head]).view(b, t, s))\n",
        "        \n",
        "        out = torch.cat(head_represenations, dim=2)\n",
        "        #transpose and recombine attention heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "        #last linear layer of weights\n",
        "        return self.recombine_heads(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, seq_len, num_token, num_inputs, num_heads, num_layers, dropout=0.3):\n",
        "      \"\"\"initiate the model\"\"\"\n",
        "      super(Transformer, self).__init__()\n",
        "      self.num_layers = num_layers\n",
        "      self.enc = nn.Embedding(num_token, num_inputs-3)\n",
        "      self.bns = torch.nn.ModuleList([nn.BatchNorm1d(seq_len) for i in range(num_layers)])\n",
        "      self.enc_transformer = torch.nn.ModuleList([MultiheadedAttention(num_inputs, seq_len,heads=num_heads, dropout=dropout) for i in range(num_layers)])\n",
        "      self.num_inputs = num_inputs\n",
        "      self.dec = nn.Linear(num_inputs, num_token)\n",
        "      pos_embeds = torch.randn((seq_len,1,3), device=device,requires_grad=True)\n",
        "      self.pos_embeds = pos_embeds.repeat(1,batch_size,1) # repeat to fit batch dimention\n",
        "\n",
        "  def forward(self, source):\n",
        "      # word embedding layer, scaled for numerical stability\n",
        "      source = self.enc(source) * math.sqrt(self.num_inputs)\n",
        "      # adding positnial embeddings\n",
        "      source = torch.cat([source, self.pos_embeds], axis=2)\n",
        "      # layers of multy head self attantion and batch-norm\n",
        "      for layer in range(self.num_layers):\n",
        "          source = source.swapaxes(0,1)\n",
        "          source = self.bns[layer](source)\n",
        "          source = source.swapaxes(0,1)\n",
        "          source = self.enc_transformer[layer](source)\n",
        "          source = source.swapaxes(0,1)\n",
        "      # decoder\n",
        "      op = self.dec(source)\n",
        "      return op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6m7q3jeZLyv"
      },
      "source": [
        "# setting veriables, building model\n",
        "seq_len = 50\n",
        "batch_size=32\n",
        "n_heads = 8\n",
        "emb_head = 30\n",
        "n_layers = 3\n",
        "m = Transformer(seq_len,len(w2i), emb_head*n_heads, n_heads, n_layers, dropout=0.0).to(device)\n",
        "loss_func = nn.BCEWithLogitsLoss() \n",
        "opt = torch.optim.Adam(m.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5E75TWSbJ4G"
      },
      "source": [
        "# data dict\n",
        "path = '/content/MusicMatrices'\n",
        "# lists for tracking performance\n",
        "all_losses = []\n",
        "all_accs = []\n",
        "# load model\n",
        "#m.load_state_dict(torch.load('/content/model.pt'))\n",
        "# set to training mode\n",
        "m.train()\n",
        "# training loop\n",
        "for epoch in range(10):\n",
        "  # print epoch num\n",
        "  print('epoch: ' + str(epoch))\n",
        "  # lists for tracking performance (within epoch)\n",
        "  losses = []\n",
        "  accs = []\n",
        "  # build dataset and dataloader\n",
        "  dataset = Dataset(seq_len, 2900)\n",
        "  generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  # initiate counter for batches trained on\n",
        "  counter = 0\n",
        "  for x, y in generator:\n",
        "    counter += 1\n",
        "    # get data ready\n",
        "    x, y = x.to(device), y.float().to(device)\n",
        "    opt.zero_grad()\n",
        "    x = x.swapaxes(1,0)\n",
        "    y = y.swapaxes(1,0)\n",
        "    # run model\n",
        "    output = m(x)\n",
        "    # sample accuracy\n",
        "    if counter % 5 == 0:\n",
        "        accs.append(torch.sum(torch.argmax(y, axis = 2) == torch.argmax(output, axis = 2)) / (output.shape[0] * output.shape[1]))\n",
        "    # calculate loss and update model\n",
        "    loss = loss_func(output, y)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    # save perfurmance\n",
        "    if counter % 100 == 0:\n",
        "        print(f'[{counter}/{len(generator)}]')\n",
        "        print(round(np.mean(losses)*10000, 2), round(np.mean([x.cpu().detach().numpy() for x in accs]), 2))\n",
        "        all_accs.append(round(np.mean([x.cpu().detach().numpy() for x in accs]), 2))\n",
        "        all_losses.append(round(np.mean(losses)*10000, 2))\n",
        "        losses = []\n",
        "        accs = []\n",
        "        print()\n",
        "# plot performance\n",
        "f, a = plt.subplots(2,1,figsize=(18,10))\n",
        "a[0].plot(all_losses)\n",
        "a[1].plot(all_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSDrOIywb994"
      },
      "source": [
        "# prepare musical prompt\n",
        "mat = np.load('/content/MusicMatrices/alb_se3.mid.npy')\n",
        "gen = mat[0:seq_len,:]\n",
        "gen = gen[:,0:88]\n",
        "l = list(range(len(w2i)))\n",
        "# length of genreated piece\n",
        "ln = int(mat.shape[0] / 8)\n",
        "# convert model to evaluation mode\n",
        "m.eval()\n",
        "# set temprature\n",
        "temp = 1.3\n",
        "# generate\n",
        "with torch.no_grad():\n",
        "    for i in range(ln):\n",
        "        # prepare data\n",
        "        start = gen[-seq_len:,:]\n",
        "        start[:,0:88][start[:,0:88] > 0] = 50\n",
        "        start = [str(j) for j in start.tolist()]\n",
        "        start = [w2i[j] for j in start]\n",
        "        start = torch.tensor(start).long().to(device).unsqueeze(0)\n",
        "        start = start.repeat(32,1)\n",
        "        start = start.swapaxes(0,1)\n",
        "        # run model\n",
        "        op = m(start)\n",
        "        # convert to probablities\n",
        "        pred = op[-1,0,:]\n",
        "        pred = pred.cpu().detach().numpy()\n",
        "        pred = pred*temp\n",
        "        pred = softmax(pred)\n",
        "        # do not choose option that are completly bizzare\n",
        "        # pred[pred<0.004] = 0 \n",
        "        # pred /= np.sum(pred)\n",
        "        # choose next words\n",
        "        pred = np.random.choice(l,p=pred)\n",
        "        # add new word to piece\n",
        "        pred = i2w[pred].replace('[','').replace('[','')\n",
        "        pred = np.fromstring(pred, sep=', ').astype(int)\n",
        "        pred = np.expand_dims(pred, 0)\n",
        "        gen = np.concatenate([gen, pred])\n",
        "\n",
        "# plot generated piece\n",
        "f, a = plt.subplots(2,1,figsize=(18,10))\n",
        "mat[mat > 1] = 1\n",
        "gen_b = gen.copy()\n",
        "gen_b[gen_b > 0] = 1\n",
        "gen_b[seq_len,:] = 1\n",
        "mat[seq_len,:] = 1\n",
        "print(gen.shape)\n",
        "a[0].imshow(gen_b.T)\n",
        "a[1].imshow(mat.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e807uIuUpNsY"
      },
      "source": [
        "# convert to MIDI and save\n",
        "a = np.expand_dims(np.array([100]*gen.shape[0]),1)\n",
        "t = np.concatenate([gen, a], axis=1)\n",
        "mat_decomp = decompress_mat(t)\n",
        "arry2mid(mat_decomp, '/content/gen.mid',tempo=1503721)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}